{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyntcloud import PyntCloud\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Reshape\n",
    "from keras.layers import Convolution1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Lambda, concatenate\n",
    "import h5py\n",
    "\n",
    "\n",
    "k=17\n",
    "adam = optimizers.Adam(lr=0.001, decay=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_mul(A,B):\n",
    "    return tf.matmul(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_dim(global_feature, num_points):\n",
    "    return tf.tile(global_feature, [1, num_points, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyntCloud\n",
      "10000000 points with 3 scalar fields\n",
      "0 faces in mesh\n",
      "0 kdtrees\n",
      "0 voxelgrids\n",
      "Centroid: -12.104286193847656, -197.56027221679688, 46.1270637512207\n",
      "Other attributes:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train=PyntCloud.from_file('/home/huhongjun/rueMadame_database/GT_Madame1_3.ply')\n",
    "num_points = train.points.shape[0]\n",
    "print(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyntCloud\n",
      "10000000 points with 3 scalar fields\n",
      "0 faces in mesh\n",
      "0 kdtrees\n",
      "0 voxelgrids\n",
      "Centroid: -7.090676784515381, -124.74476623535156, 45.12932205200195\n",
      "Other attributes:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test=PyntCloud.from_file('/home/huhongjun/rueMadame_database/GT_Madame1_2.ply')\n",
    "num_points = test.points.shape[0]\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/huhongjun/anaconda2/envs/tfgpu-1-10/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1154: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/huhongjun/anaconda2/envs/tfgpu-1-10/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1188: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 10000000, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 10000000, 3)   0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 10000000, 64)  256         lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 10000000, 64)  256         conv1d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 10000000, 64)  4160        batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 10000000, 64)  256         conv1d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 10000000, 64)  0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)                (None, 10000000, 64)  4160        lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 10000000, 64)  256         conv1d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)               (None, 10000000, 128) 8320        batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 10000000, 128) 512         conv1d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)               (None, 10000000, 1024 132096      batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, 10000000, 1024 4096        conv1d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 1, 1024)       0           batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 10000000, 1024 0           max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 10000000, 1088 0           lambda_2[0][0]                   \n",
      "                                                                   lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)               (None, 10000000, 512) 557568      concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 10000000, 512) 2048        conv1d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)               (None, 10000000, 256) 131328      batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 10000000, 256) 1024        conv1d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)               (None, 10000000, 128) 32896       batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNor (None, 10000000, 128) 512         conv1d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)               (None, 10000000, 128) 16512       batch_normalization_18[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNor (None, 10000000, 128) 512         conv1d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)               (None, 10000000, 17)  2193        batch_normalization_19[0][0]     \n",
      "====================================================================================================\n",
      "Total params: 898,961\n",
      "Trainable params: 894,225\n",
      "Non-trainable params: 4,736\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#input_transformation_net\n",
    "input_points = Input(shape=(num_points, 3))\n",
    "x = Convolution1D(64, 1, activation='relu',input_shape=(num_points, 3))(input_points)\n",
    "x = BatchNormalization()(x)\n",
    "x = Convolution1D(128, 1, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Convolution1D(1024, 1, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=num_points)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(9, weights=[np.zeros([256, 9]), np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32)])(x)\n",
    "input_T = Reshape((3, 3))(x)\n",
    "#forward_net\n",
    "g = Lambda(mat_mul, arguments={'B': input_T})(input_points)\n",
    "g = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\n",
    "g = BatchNormalization()(g)\n",
    "g = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\n",
    "g = BatchNormalization()(g)\n",
    "#feature_transformation_net\n",
    "f = Convolution1D(64, 1, activation='relu')(g)\n",
    "f = BatchNormalization()(f)\n",
    "f = Convolution1D(128, 1, activation='relu')(f)\n",
    "f = BatchNormalization()(f)\n",
    "f = Convolution1D(1024, 1, activation='relu')(f)\n",
    "f = BatchNormalization()(f)\n",
    "f = MaxPooling1D(pool_size=num_points)(f)\n",
    "f = Dense(512, activation='relu')(f)\n",
    "f = BatchNormalization()(f)\n",
    "f = Dense(256, activation='relu')(f)\n",
    "f = BatchNormalization()(f)\n",
    "f = Dense(64 * 64, weights=[np.zeros([256, 64 * 64]), np.eye(64).flatten().astype(np.float32)])(f)\n",
    "feature_T = Reshape((64, 64))(f)\n",
    "\n",
    "# forward net\n",
    "g = Lambda(mat_mul, arguments={'B': feature_T})(g)\n",
    "seg_part1 = g\n",
    "g = Convolution1D(64, 1, activation='relu')(g)\n",
    "g = BatchNormalization()(g)\n",
    "g = Convolution1D(128, 1, activation='relu')(g)\n",
    "g = BatchNormalization()(g)\n",
    "g = Convolution1D(1024, 1, activation='relu')(g)\n",
    "g = BatchNormalization()(g)\n",
    "\n",
    "# global_feature\n",
    "global_feature = MaxPooling1D(pool_size=num_points)(g)\n",
    "global_feature = Lambda(exp_dim, arguments={'num_points': num_points})(global_feature)\n",
    "\n",
    "# point_net_seg\n",
    "c = concatenate([seg_part1, global_feature])\n",
    "c = Convolution1D(512, 1, activation='relu')(c)\n",
    "c = BatchNormalization()(c)\n",
    "c = Convolution1D(256, 1, activation='relu')(c)\n",
    "c = BatchNormalization()(c)\n",
    "c = Convolution1D(128, 1, activation='relu')(c)\n",
    "c = BatchNormalization()(c)\n",
    "c = Convolution1D(128, 1, activation='relu')(c)\n",
    "c = BatchNormalization()(c)\n",
    "prediction = Convolution1D(k, 1, activation='softmax')(c)\n",
    "'''\n",
    "end of pointnet\n",
    "'''\n",
    "\n",
    "# define model\n",
    "model = Model(inputs=input_points, outputs=prediction)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.points.drop(labels=[\"label\",\"reflectance\"],inplace=True,axis=1)\n",
    "train_labels=train.points[\"class\"]\n",
    "train.points.drop(labels=[\"class\"],inplace=True,axis=1)\n",
    "train_points = train.points.values.reshape(-1, num_points, 3)\n",
    "train_labels=dense_to_one_hot(train_labels,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.points.drop(labels=[\"label\",\"reflectance\"],inplace=True,axis=1)\n",
    "test_labels=test.points[\"class\"]\n",
    "test.points.drop(labels=[\"class\"],inplace=True,axis=1)\n",
    "test_points = test.points.values.reshape(-1, num_points, 3)\n",
    "test_labels=dense_to_one_hot(test_labels,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000000, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 17)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000000, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 17)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array([train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000000, 17)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels=np.array([test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000000, 17)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected conv1d_16 to have 3 dimensions, but got array with shape (10000000, 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9392c70a05d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# rotate and jitter point cloud every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#score = model.evaluate(test_points_r, test_labels_r, verbose=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected conv1d_16 to have 3 dimensions, but got array with shape (10000000, 17)"
     ]
    }
   ],
   "source": [
    "# rotate and jitter point cloud every epoch\n",
    "model.fit(train_points, train_labels, batch_size=1, epochs=50, shuffle=True, verbose=1)    \n",
    "\n",
    "score = model.evaluate(test_points, test_label50, verbose=1)\n",
    "print('Test loss: ', score[0])\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7(Urban)",
   "language": "python",
   "name": "urban"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
